{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yashpapa6969/data-venki/blob/main/transformer_SMOE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb3yFHWeRre1",
        "outputId": "6730eb62-4779-4199-9171-3a9e61389dc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.4.0.post0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.17.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.3.0+cu121)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.11.3.post0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.9.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.6.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics) (12.5.82)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets torchmetrics wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8B20XFJUlR_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import einsum\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional\n",
        "from typing import Dict,Tuple\n",
        "from typing import List, Union\n",
        "from collections.abc import MutableMapping\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cl5Owr4QJ1t",
        "outputId": "92529f37-ac94-4b85-9693-9c2404e193a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "384.0"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "768/2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOl3sL1QOIgG"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ModelConfig:\n",
        "  d_model :int = 768\n",
        "  n_heads :int = 12\n",
        "  ffn_dim_multiplier: Optional[float] = None\n",
        "  multiple_of: int = 256\n",
        "  dropout: float = 0.1\n",
        "  pad_token_id: int = 1\n",
        "  hidden_size: int = 768\n",
        "  num_attention_heads:int = 12\n",
        "  max_sequence_length: int = 768\n",
        "\n",
        "  num_experts: int = 4\n",
        "  num_experts_per_tok: int = 2\n",
        "  capacity_factor: float = 1.25\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRCyDFDpUlOt"
      },
      "outputs": [],
      "source": [
        "# class LayerNormalization(nn.Module):\n",
        "\n",
        "#     def __init__(self, features: int, eps:float=10**-6) -> None:\n",
        "#         super().__init__()\n",
        "#         self.eps = eps\n",
        "#         # Here features are the d_model\n",
        "#         self.alpha = nn.Parameter(torch.ones(features)) # alpha is a learnable parameter\n",
        "#         self.bias = nn.Parameter(torch.zeros(features)) # bias is a learnable parameter\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # x: (batch, seq_len, hidden_size)\n",
        "#          # Keep the dimension for broadcasting\n",
        "#         mean = x.mean(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
        "#         # Keep the dimension for broadcasting\n",
        "#         std = x.std(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
        "#         # eps is to prevent dividing by zero or when std is very small\n",
        "#         return self.alpha * (x - mean) / (std + self.eps) + self.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gz8TmUBxG3eb"
      },
      "outputs": [],
      "source": [
        "class BufferCache(dict, MutableMapping[str, torch.Tensor]):\n",
        "  \"\"\" \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgCr2dAaFY9H"
      },
      "outputs": [],
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        # The gamma parameter\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def _norm(self, x: torch.Tensor):\n",
        "        # (B, Seq_Len, Dim) * (B, Seq_Len, 1) = (B, Seq_Len, Dim)\n",
        "        # rsqrt: 1 / sqrt(x)\n",
        "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        # (Dim) * (B, Seq_Len, Dim) = (B, Seq_Len, Dim)\n",
        "        return self.weight * self._norm(x.float()).type_as(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9mT2efTUlMB"
      },
      "outputs": [],
      "source": [
        "# class FeedForwardBlock(nn.Module):\n",
        "\n",
        "#     def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
        "#         super().__init__()\n",
        "#         self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "#         self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
        "#         return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsXtx6UcSHy5"
      },
      "outputs": [],
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        args: ModelConfig\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        hidden_dim = 4 * args.d_model\n",
        "        hidden_dim = int(2 * hidden_dim / 3)\n",
        "        if args.ffn_dim_multiplier is not None:\n",
        "            hidden_dim = int(args.ffn_dim_multiplier * hidden_dim)\n",
        "        # Round the hidden_dim to the nearest multiple of the multiple_of parameter\n",
        "        hidden_dim = args.multiple_of * ((hidden_dim + args.multiple_of - 1) // args.multiple_of)\n",
        "\n",
        "        self.w1 = nn.Linear(args.d_model, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(hidden_dim, args.d_model, bias=False)\n",
        "        self.w3 = nn.Linear(args.d_model, hidden_dim, bias=False)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        # (B, Seq_Len, Dim) --> (B, Seq_Len, Hidden_Dim)\n",
        "        swish = F.silu(self.w1(x))\n",
        "        # (B, Seq_Len, Dim) --> (B, Seq_Len, Hidden_Dim)\n",
        "        x_V = self.w3(x)\n",
        "        # (B, Seq_Len, Hidden_Dim) * (B, Seq_Len, Hidden_Dim) --> (B, Seq_Len, Hidden_Dim)\n",
        "        x = swish * x_V\n",
        "        # (B, Seq_Len, Hidden_Dim) --> (B, Seq_Len, Dim)\n",
        "        x = self.w2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Xvv2qV5UlJc"
      },
      "outputs": [],
      "source": [
        "class InputEmbeddings(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch, seq_len) --> (batch, seq_len, d_model)\n",
        "        # Multiply by sqrt(d_model) to scale the embeddings according to the paper\n",
        "        return self.embedding(x) * math.sqrt(self.d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72T_p0OCUlG7"
      },
      "outputs": [],
      "source": [
        "# class PositionalEncoding(nn.Module):\n",
        "\n",
        "#     def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
        "#         super().__init__()\n",
        "#         self.d_model = d_model\n",
        "#         self.seq_len = seq_len\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "#         # Create a matrix of shape (seq_len, d_model)\n",
        "#         pe = torch.zeros(seq_len, d_model)\n",
        "#         # Create a vector of shape (seq_len)\n",
        "#         position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
        "#         # Create a vector of shape (d_model)\n",
        "#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
        "#         # Apply sine to even indices\n",
        "#         pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
        "#         # Apply cosine to odd indices\n",
        "#         pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
        "#         # Add a batch dimension to the positional encoding\n",
        "#         pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
        "#         # Register the positional encoding as a buffer\n",
        "#         self.register_buffer('pe', pe)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
        "#         return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-r-MudFHZvQ"
      },
      "outputs": [],
      "source": [
        "class RotaryEmbedding(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, config: ModelConfig, cache: BufferCache, device):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.__cache = cache\n",
        "\n",
        "        # Warm up cache.\n",
        "        self.get_rotary_embedding(config.max_sequence_length, device)\n",
        "\n",
        "    def get_rotary_embedding(self, seq_len: int, device: torch.device) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        if (\n",
        "            (pos_sin := self.__cache.get(\"rope_pos_sin\")) is not None\n",
        "            and (pos_cos := self.__cache.get(\"rope_pos_cos\")) is not None\n",
        "            and pos_sin.shape[-2] >= seq_len\n",
        "            and pos_cos.shape[-2] >= seq_len\n",
        "        ):\n",
        "            if pos_sin.device != device:\n",
        "                pos_sin = pos_sin.to(device)\n",
        "                self.__cache[\"rope_pos_sin\"] = pos_sin\n",
        "            if pos_cos.device != device:\n",
        "                pos_cos = pos_cos.to(device)\n",
        "                self.__cache[\"rope_pos_cos\"] = pos_cos\n",
        "            return pos_sin[:, :, :seq_len, :], pos_cos[:, :, :seq_len, :]\n",
        "\n",
        "        dim = self.config.d_model // self.config.n_heads\n",
        "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2, device=device, dtype=torch.float) / dim))\n",
        "        seq = torch.arange(seq_len, device=device, dtype=torch.float)\n",
        "        freqs = einsum(\"i , j -> i j\", seq, inv_freq)\n",
        "        positions = torch.cat((freqs, freqs), dim=-1)\n",
        "        pos_sin, pos_cos = positions.sin()[None, None, :, :], positions.cos()[None, None, :, :]\n",
        "        self.__cache[\"rope_pos_sin\"] = pos_sin\n",
        "        self.__cache[\"rope_pos_cos\"] = pos_cos\n",
        "        return pos_sin, pos_cos\n",
        "\n",
        "    def rotate_half(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        B, nh, T, hs = x.size()\n",
        "        x = x.view(B, nh, T, 2, hs // 2)\n",
        "        x1, x2 = x.unbind(dim=-2)\n",
        "        return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "    def apply_rotary_pos_emb(self, pos_sin: torch.Tensor, pos_cos: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
        "        return ((t * pos_cos) + (self.rotate_half(t) * pos_sin)).to(t.dtype)\n",
        "\n",
        "    def forward(self, q: torch.Tensor, k: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        q_, k_ = q, k\n",
        "        query_len, key_len = q_.shape[-2], k_.shape[-2]  # could be different if layer_past not None\n",
        "        pos_sin, pos_cos = self.get_rotary_embedding(key_len, q_.device)\n",
        "        pos_sin = pos_sin.type_as(q_)\n",
        "        pos_cos = pos_cos.type_as(q_)\n",
        "        q_ = self.apply_rotary_pos_emb(\n",
        "            pos_sin[:, :, key_len - query_len : key_len, :],\n",
        "            pos_cos[:, :, key_len - query_len : key_len, :],\n",
        "            q_,\n",
        "        )\n",
        "        k_ = self.apply_rotary_pos_emb(pos_sin, pos_cos, k_)\n",
        "        return q_.type_as(q), k_.type_as(k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExZU6bHRUlEN"
      },
      "outputs": [],
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "\n",
        "        def __init__(self, features: int, dropout: float) -> None:\n",
        "            super().__init__()\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "            self.norm = RMSNorm(features)\n",
        "\n",
        "        def forward(self, x, sublayer):\n",
        "            return x + self.dropout(sublayer(self.norm(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SAzka3s4SbQ"
      },
      "outputs": [],
      "source": [
        "class MoeLayer(nn.Module):\n",
        "    def __init__(self, experts: List[nn.Module], gate: nn.Module, moe_args: ModelConfig):\n",
        "        super().__init__()\n",
        "        assert len(experts) > 0\n",
        "        self.experts = nn.ModuleList(experts)\n",
        "        self.gate = gate\n",
        "        self.args = moe_args\n",
        "        self.balance_loss = None\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
        "        batch_size, seq_len, d_model = inputs.shape\n",
        "        flat_inputs = inputs.view(-1, d_model)\n",
        "\n",
        "        # Generate gate logits\n",
        "        gate_logits = self.gate(flat_inputs)\n",
        "\n",
        "        # Select top-k experts\n",
        "        weights, selected_experts = torch.topk(\n",
        "            gate_logits, self.args.num_experts_per_tok, dim=-1\n",
        "        )\n",
        "        weights = F.softmax(weights, dim=-1, dtype=torch.float).to(inputs.dtype)\n",
        "\n",
        "        # Calculate expert capacity\n",
        "        num_tokens = batch_size * seq_len\n",
        "        capacity = int(self.args.capacity_factor * num_tokens // self.args.num_experts)\n",
        "\n",
        "        # Compute load balancing\n",
        "        expert_counts = torch.zeros(self.args.num_experts, device=inputs.device)\n",
        "        for i in range(self.args.num_experts_per_tok):\n",
        "            expert_counts += torch.bincount(selected_experts[:, i], minlength=self.args.num_experts)\n",
        "\n",
        "        # Compute balance loss\n",
        "        mean_count = num_tokens / self.args.num_experts\n",
        "        self.balance_loss = torch.sum((expert_counts - mean_count)**2) / (self.args.num_experts * mean_count**2)\n",
        "\n",
        "        results = torch.zeros_like(flat_inputs)\n",
        "        for i, expert in enumerate(self.experts):\n",
        "            # Find tokens routed to this expert\n",
        "            expert_mask = (selected_experts == i)\n",
        "            token_indices = expert_mask.nonzero(as_tuple=True)[0]\n",
        "            expert_weights = weights[expert_mask]\n",
        "\n",
        "            # Apply capacity constraint\n",
        "            if token_indices.shape[0] > capacity:\n",
        "                perm = torch.randperm(token_indices.shape[0], device=token_indices.device)\n",
        "                token_indices = token_indices[perm[:capacity]]\n",
        "                expert_weights = expert_weights[perm[:capacity]]\n",
        "\n",
        "            if token_indices.shape[0] == 0:\n",
        "                continue\n",
        "\n",
        "            # Compute expert outputs\n",
        "            expert_inputs = flat_inputs[token_indices]\n",
        "            expert_outputs = expert(expert_inputs)\n",
        "\n",
        "            # Combine expert outputs weighted by gate probabilities\n",
        "            results.index_add_(0, token_indices, expert_weights.unsqueeze(-1) * expert_outputs)\n",
        "\n",
        "        return results.view(batch_size, seq_len, d_model)\n",
        "\n",
        "    def get_balance_loss(self):\n",
        "        return self.balance_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxmNxr3QMi5N"
      },
      "outputs": [],
      "source": [
        "\n",
        "class SelfAttentionBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, h: int, dropout: float, config: ModelConfig, cache: BufferCache, device) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model # Embedding vector size\n",
        "        self.h = h # Number of heads\n",
        "        # Make sure d_model is divisible by h\n",
        "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
        "\n",
        "        self.d_k = d_model // h # Dimension of vector seen by each head\n",
        "        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n",
        "        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n",
        "        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n",
        "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.__cache = cache\n",
        "        self.rotary_emb = RotaryEmbedding(config, self.__cache, device)\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
        "        d_k = query.shape[-1]\n",
        "        # Just apply the formula from the paper\n",
        "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
        "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "        if mask is not None:\n",
        "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
        "            attention_scores.masked_fill_(mask == 0, torch.finfo(attention_scores.dtype).min)\n",
        "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
        "        if dropout is not None:\n",
        "            attention_scores = dropout(attention_scores)\n",
        "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
        "        # return attention scores which can be used for visualization\n",
        "        return (attention_scores @ value), attention_scores\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
        "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "\n",
        "        query, key = self.rotary_emb(query, key)   #Rotary positional Embedding\n",
        "\n",
        "        # Calculate attention\n",
        "        x, self.attention_scores = SelfAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "        # Combine all the heads together\n",
        "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
        "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
        "\n",
        "        # Multiply by Wo\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        return self.w_o(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQHkiJrnUlBk"
      },
      "outputs": [],
      "source": [
        "class CrossAttentionBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model # Embedding vector size\n",
        "        self.h = h # Number of heads\n",
        "        # Make sure d_model is divisible by h\n",
        "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
        "\n",
        "        self.d_k = d_model // h # Dimension of vector seen by each head\n",
        "        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n",
        "        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n",
        "        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n",
        "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
        "        d_k = query.shape[-1]\n",
        "        # Just apply the formula from the paper\n",
        "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
        "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "        if mask is not None:\n",
        "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
        "            attention_scores.masked_fill_(mask == 0, torch.finfo(attention_scores.dtype).min)\n",
        "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
        "        if dropout is not None:\n",
        "            attention_scores = dropout(attention_scores)\n",
        "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
        "        # return attention scores which can be used for visualization\n",
        "        return (attention_scores @ value), attention_scores\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
        "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Calculate attention\n",
        "        x, self.attention_scores = CrossAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "        # Combine all the heads together\n",
        "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
        "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
        "\n",
        "        # Multiply by Wo\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        return self.w_o(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hj5-2IIjUk_G"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, self_attention_block: SelfAttentionBlock, feed_forward_block: MoeLayer, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
        "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vs5zIuZCTG0O"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = RMSNorm(features)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OkT7PAkUk6J"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, self_attention_block: SelfAttentionBlock, cross_attention_block: CrossAttentionBlock, feed_forward_block: MoeLayer, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.cross_attention_block = cross_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
        "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
        "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HH3X_fhuUk3r"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = RMSNorm(features)\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "        return self.norm(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TY26saumUk1Q"
      },
      "outputs": [],
      "source": [
        "class ProjectionLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, vocab_size) -> None:\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x) -> None:\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
        "        return self.proj(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEUyOfo-Ukyv"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings,  projection_layer: ProjectionLayer) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.projection_layer = projection_layer\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        # (batch, seq_len, d_model)\n",
        "        src = self.src_embed(src)\n",
        "        return self.encoder(src, src_mask)\n",
        "\n",
        "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
        "        # (batch, seq_len, d_model)\n",
        "        tgt = self.tgt_embed(tgt)\n",
        "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "    def project(self, x):\n",
        "        # (batch, seq_len, vocab_size)\n",
        "        return self.projection_layer(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcL2AnG8UkwQ"
      },
      "outputs": [],
      "source": [
        "def build_transformer(src_vocab_size: int, tgt_vocab_size: int,  d_model: int=768, N: int=6, h: int=12, dropout: float=0.1) -> Transformer:\n",
        "    # Create the embedding layers\n",
        "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
        "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
        "\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n",
        "    # Create the encoder blocks\n",
        "    Cache = BufferCache()\n",
        "    encoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        encoder_self_attention_block = SelfAttentionBlock(d_model, h, dropout, ModelConfig, Cache,device)\n",
        "        feed_forward_block = MoeLayer(\n",
        "                experts=[FeedForwardBlock(ModelConfig) for _ in range(ModelConfig.num_experts)],\n",
        "                gate=nn.Linear(ModelConfig.d_model, ModelConfig.num_experts, bias=False),\n",
        "                moe_args=ModelConfig,\n",
        "            )\n",
        "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
        "        encoder_blocks.append(encoder_block)\n",
        "\n",
        "    # Create the decoder blocks\n",
        "    decoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        decoder_self_attention_block = SelfAttentionBlock(d_model, h, dropout, ModelConfig, Cache,device)\n",
        "        decoder_cross_attention_block = CrossAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = MoeLayer(\n",
        "                experts=[FeedForwardBlock(ModelConfig) for _ in range(ModelConfig.num_experts)],\n",
        "                gate=nn.Linear(ModelConfig.d_model, ModelConfig.num_experts, bias=False),\n",
        "                moe_args=ModelConfig,\n",
        "            )\n",
        "        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
        "        decoder_blocks.append(decoder_block)\n",
        "\n",
        "    # Create the encoder and decoder\n",
        "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
        "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
        "\n",
        "    # Create the projection layer\n",
        "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
        "\n",
        "    # Create the transformer\n",
        "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, projection_layer)\n",
        "\n",
        "    # Initialize the parameters\n",
        "    for p in transformer.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    return transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TArL4_iszBQ"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def get_config():\n",
        "    return {\n",
        "        \"batch_size\": 24,\n",
        "        \"num_epochs\": 3,\n",
        "        \"lr\": 2e-4,\n",
        "        \"seq_len\": 320,\n",
        "        \"d_model\": 768,\n",
        "        \"datasource\": \"ai4bharat/IndicParaphrase\",\n",
        "        \"lang_src\": \"en\",\n",
        "        \"lang_tgt\": \"hi\",\n",
        "        \"model_folder\": \"weights\",\n",
        "        \"model_basename\": \"tmodel_\",\n",
        "        \"preload\": \"latest\",\n",
        "        \"tokenizer_file\": \"tokenizer_{0}.json\",\n",
        "        \"experiment_name\": \"runs/tmodel\"\n",
        "    }\n",
        "\n",
        "def get_weights_file_path(config, epoch: str):\n",
        "    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n",
        "    model_filename = f\"{config['model_basename']}{epoch}.pt\"\n",
        "    return str(Path('.') / model_folder / model_filename)\n",
        "\n",
        "# Find the latest weights file in the weights folder\n",
        "def latest_weights_file_path(config):\n",
        "    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n",
        "    model_filename = f\"{config['model_basename']}*\"\n",
        "    weights_files = list(Path(model_folder).glob(model_filename))\n",
        "    if len(weights_files) == 0:\n",
        "        return None\n",
        "    weights_files.sort()\n",
        "    return str(weights_files[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7Uzfj1pdL7L"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, concatenate_datasets, Dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myhjpYPkpGVB"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers import SentencePieceBPETokenizer\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQTFQsTUj0OG"
      },
      "outputs": [],
      "source": [
        "def format_dataset(example):\n",
        "     return {\"en\": example[\"src\"],\n",
        "             \"hi\": example[\"tgt\"]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gE2v4IhcmQ5p"
      },
      "outputs": [],
      "source": [
        "def preprocess_dataset(dataset):\n",
        "  dataset = load_dataset(dataset)\n",
        "  all_dataset = concatenate_datasets([dataset[split] for split in dataset.keys()])\n",
        "  df = all_dataset.to_pandas()\n",
        "  df[\"translation\"] = df[[\"src\", \"tgt\"]].apply(lambda x: format_dataset(x), axis=1)\n",
        "  df = df.drop(columns=[\"src\", \"tgt\"], axis=1)\n",
        "  hf_dataset = Dataset.from_pandas(df)\n",
        "  return hf_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtVGnZkcm_TW"
      },
      "outputs": [],
      "source": [
        "dataset = \"Venkatesh4342/samanantar-hi-seq-len-300\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2KrB_YYnLK9",
        "outputId": "a731cb9d-f791-48d3-bc1a-5d3187eafeed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "ff_dataset = preprocess_dataset(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94-tISwpnTBX",
        "outputId": "41f5a19d-3ab3-43a5-985a-1c514a5dd2c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'en': 'There is also considerable potential for Russia and India to reinforce each other, in executing energy and rail transportation projects in third countries, like Afghanistan and Vietnam.',\n",
              " 'hi': 'भारत-रूस के बीच तीसरे देशों जैसे कि अफगानिस्तान और वियतनाम में ऊर्जा और रेल परिवहन परियोजनाओं में एक-दूसरे का सहयोग करते हुए कमाई करने की काफी संभावना है।'}"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ff_dataset[\"translation\"][500009]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zs0beHLof-w9"
      },
      "outputs": [],
      "source": [
        "def get_all_sentences(ds, lang):\n",
        "    for item in ds:\n",
        "        yield item['translation'][lang]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVN7f__KfemT"
      },
      "outputs": [],
      "source": [
        "def get_or_build_tokenizer(config, ds, lang):\n",
        "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
        "    if not Path.exists(tokenizer_path):\n",
        "        if lang == \"en\":\n",
        "            vocab_size = 32_000\n",
        "        elif lang == \"hi\":\n",
        "            vocab_size = 64_000\n",
        "\n",
        "        # Most code taken from: https://huggingface.co/docs/tokenizers/quicktour\n",
        "        #tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
        "        tokenizer = SentencePieceBPETokenizer()\n",
        "        #trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
        "        special_tokens = [\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"]\n",
        "        tokenizer.train_from_iterator(get_all_sentences(ds, lang),\n",
        "        vocab_size=vocab_size,\n",
        "        min_frequency=5,\n",
        "        special_tokens=special_tokens,\n",
        "        show_progress=True,)\n",
        "        tokenizer.save(str(tokenizer_path))\n",
        "    else:\n",
        "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
        "    return tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPTqAv0aBVDg"
      },
      "outputs": [],
      "source": [
        "config  = get_config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAltSvrGBJ8M",
        "outputId": "79edd9d9-f75c-4474-eb19-f82b77959f20"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Path.exists(Path(config['tokenizer_file'].format(\"en\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2JH9-VMx8NA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Huggingface datasets and tokenizers\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "import torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98mTRjDPfvqO"
      },
      "outputs": [],
      "source": [
        "def get_ds(config):\n",
        "    # It only has the train split, so we divide it overselves\n",
        "    ds_raw = ff_dataset\n",
        "\n",
        "    # Build tokenizers\n",
        "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src'])\n",
        "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
        "\n",
        "    # Keep 90% for training, 10% for validation\n",
        "    train_ds_size = int(0.001 * len(ds_raw))\n",
        "    val_ds_size = len(ds_raw) - train_ds_size\n",
        "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
        "\n",
        "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "\n",
        "    # Find the maximum length of each sentence in the source and target sentence\n",
        "    max_len_src = 0\n",
        "    max_len_tgt = 0\n",
        "    src_list = []\n",
        "    tgt_list = []\n",
        "    src_str = []\n",
        "    tgt_str = []\n",
        "\n",
        "    # for item in ds_raw:\n",
        "    #     src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
        "    #     tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n",
        "    #     src_list.append(len(src_ids))\n",
        "    #     tgt_list.append(len(tgt_ids))\n",
        "    #     src_str.append(item['translation'][config['lang_src']])\n",
        "    #     tgt_str.append(item['translation'][config['lang_tgt']])\n",
        "    #     max_len_src = max(max_len_src, len(src_ids))\n",
        "    #     max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
        "\n",
        "    print(f'Max length of source sentence: {max_len_src}')\n",
        "    print(f'Max length of target sentence: {max_len_tgt}')\n",
        "\n",
        "\n",
        "    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
        "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
        "\n",
        "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt, {\"src_len\": src_list, \"tgt_len\":tgt_list, \"src\":src_str, \"tgt\":tgt_str}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6L4K7hntVsE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class BilingualDataset(Dataset):\n",
        "\n",
        "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        self.ds = ds\n",
        "        self.tokenizer_src = tokenizer_src\n",
        "        self.tokenizer_tgt = tokenizer_tgt\n",
        "        self.src_lang = src_lang\n",
        "        self.tgt_lang = tgt_lang\n",
        "\n",
        "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"<sos>\")], dtype=torch.int64)\n",
        "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"<eos>\")], dtype=torch.int64)\n",
        "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"<pad>\")], dtype=torch.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_target_pair = self.ds[idx]\n",
        "        src_text = src_target_pair['translation'][self.src_lang]\n",
        "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
        "\n",
        "        # Transform the text into tokens\n",
        "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
        "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
        "\n",
        "        # Add sos, eos and padding to each sentence\n",
        "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2  # We will add <s> and </s>\n",
        "        # We will only add <s>, and </s> only on the label\n",
        "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
        "\n",
        "        # Make sure the number of padding tokens is not negative. If it is, the sentence is too long\n",
        "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
        "            raise ValueError(\"Sentence is too long\")\n",
        "\n",
        "        # Add <s> and </s> token\n",
        "        encoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token,\n",
        "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        # Add only <s> token\n",
        "        decoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token,\n",
        "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        # Add only </s> token\n",
        "        label = torch.cat(\n",
        "            [\n",
        "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        # Double check the size of the tensors to make sure they are all seq_len long\n",
        "        assert encoder_input.size(0) == self.seq_len\n",
        "        assert decoder_input.size(0) == self.seq_len\n",
        "        assert label.size(0) == self.seq_len\n",
        "\n",
        "        return {\n",
        "            \"encoder_input\": encoder_input ,  # (seq_len)\n",
        "            \"decoder_input\": decoder_input,  # (seq_len)\n",
        "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)\n",
        "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, seq_len) & (1, seq_len, seq_len),\n",
        "            \"label\": label,  # (seq_len)\n",
        "            \"src_text\": src_text,\n",
        "            \"tgt_text\": tgt_text,\n",
        "        }\n",
        "\n",
        "def causal_mask(size):\n",
        "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
        "    return mask == 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwl0W2X86lrC"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import warnings\n",
        "from abc import abstractmethod\n",
        "from inspect import signature\n",
        "from typing import Any, Callable, Dict, List, Optional, Tuple, TypeVar, cast\n",
        "\n",
        "import torch\n",
        "import wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "E3apxSqBZtof",
        "outputId": "d4b4db23-ddbb-4baa-8c05-6683856ee4b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvenkatesh3132003\u001b[0m (\u001b[33mnlp_explorer\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "S-Tnfq5gx4gk"
      },
      "outputs": [],
      "source": [
        "StateType = Dict[str, torch.Tensor]\n",
        "StepFunctionTypeWithTimestep = Callable[[torch.Tensor, StateType, int], Tuple[torch.Tensor, StateType]]\n",
        "StepFunctionTypeNoTimestep = Callable[[torch.Tensor, StateType], Tuple[torch.Tensor, StateType]]\n",
        "\n",
        "StepFunctionType = TypeVar(\"StepFunctionType\", StepFunctionTypeWithTimestep, StepFunctionTypeNoTimestep)\n",
        "ConstraintStateType = List[List[Dict[str, Any]]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FiCsJr7-yBqK"
      },
      "outputs": [],
      "source": [
        "class Sampler:\n",
        "\n",
        "    def init_state(\n",
        "        self, start_class_log_probabilities: torch.Tensor, batch_size: int, num_classes: int\n",
        "    ) -> StateType:\n",
        "        del start_class_log_probabilities, batch_size, num_classes\n",
        "        return {}\n",
        "\n",
        "    @abstractmethod\n",
        "    def sample_nodes(\n",
        "        self, log_probs: torch.Tensor, per_node_beam_size: int, state: StateType\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor, StateType]:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def sample_beams(\n",
        "        self, log_probs: torch.Tensor, beam_size: int, state: StateType\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor, StateType]:\n",
        "        del state\n",
        "        selected_log_probs, selected_indices = torch.topk(log_probs, beam_size, dim=-1)\n",
        "        return selected_log_probs, selected_indices, {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3o5fgVDCyR9m"
      },
      "outputs": [],
      "source": [
        "class TopKSampler(Sampler):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        k: int = 1,\n",
        "        temperature: float = 1.0,\n",
        "        with_replacement: bool = False,\n",
        "    ):\n",
        "        self.k = k\n",
        "        self.temperature = temperature or 1.0\n",
        "        self.with_replacement = with_replacement\n",
        "\n",
        "    def sample_nodes(\n",
        "        self, log_probs: torch.Tensor, per_node_beam_size: int, state: StateType\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor, StateType]:\n",
        "        if not per_node_beam_size <= self.k <= log_probs.size()[1]:\n",
        "            raise ValueError(\n",
        "                \"k must be a postive integer no less than per_node_beam_size and no greater than vocabulary size\"\n",
        "            )\n",
        "\n",
        "        # shape (both): (batch_size, k)\n",
        "        top_k_log_probs, top_k_indices = log_probs.topk(self.k, dim=-1)\n",
        "\n",
        "        # Apply temperature if necessary.\n",
        "        # shape: (batch_size, k)\n",
        "        if self.temperature != 1.0:\n",
        "            top_k_log_probs = top_k_log_probs / self.temperature\n",
        "\n",
        "        # Re-normalize the subset.\n",
        "        # shape: (batch_size, k)\n",
        "        normalized_top_k_probs = torch.nn.functional.softmax(top_k_log_probs, dim=-1)\n",
        "\n",
        "        # Sample from the re-normalized subset.\n",
        "        # NOTE: These indices are not indices into `log_probs`, they are indices into `top_k_log_probs`.\n",
        "        # shape: (batch_size, per_node_beam_size)\n",
        "        sampled_indices = torch.multinomial(\n",
        "            normalized_top_k_probs, per_node_beam_size, replacement=self.with_replacement\n",
        "        )\n",
        "\n",
        "        # Convert `sampled_indices` back to indices in the original `log_probs` tensor.\n",
        "        # shape: (batch_size, per_node_beam_size)\n",
        "        indices = top_k_indices.gather(-1, sampled_indices)\n",
        "\n",
        "        return log_probs.gather(1, indices), indices, state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jsKq_avcyZuZ"
      },
      "outputs": [],
      "source": [
        "class TopPSampler(Sampler):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        p: float = 0.9,\n",
        "        temperature: float = 1.0,\n",
        "        with_replacement: bool = False,\n",
        "    ):\n",
        "        if p < 0.0 or p > 1.0:\n",
        "            raise ValueError(\"p must be a positive float no greater than 1.0\")\n",
        "        self.p = p\n",
        "        self.temperature = temperature or 1.0\n",
        "        self.with_replacement = with_replacement\n",
        "\n",
        "    def sample_nodes(\n",
        "        self, log_probs: torch.Tensor, per_node_beam_size: int, state: StateType\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor, StateType]:\n",
        "        if not per_node_beam_size <= log_probs.size()[1]:\n",
        "            raise ValueError(\"per_node_beam_size cannot be greater than vocabulary size\")\n",
        "\n",
        "        # First apply temperature coefficient:\n",
        "        if self.temperature != 1.0:\n",
        "            _log_probs = torch.nn.functional.log_softmax(log_probs / self.temperature, dim=-1)\n",
        "        else:\n",
        "            _log_probs = log_probs\n",
        "\n",
        "        # Sort the probabilities in descending order to then find cumulative sum\n",
        "        log_probs_descending, sorting_indices = torch.sort(_log_probs, descending=True)\n",
        "\n",
        "        # shape: (batch_size, num_classes)\n",
        "        probabilities_descending = log_probs_descending.exp()\n",
        "        probabilities_summed = torch.cumsum(probabilities_descending, dim=-1)\n",
        "\n",
        "        # Create a mask for filtering out probabilities that don't make the top `p`.\n",
        "        # shape: (batch_size, num_classes)\n",
        "        exclusion_mask = probabilities_summed >= self.p\n",
        "\n",
        "        # We want to include the first index where probabilities_summed >= p, so we shift over one.\n",
        "        exclusion_mask[..., 1:] = exclusion_mask[..., :-1].clone()\n",
        "        exclusion_mask[..., 0] = False\n",
        "\n",
        "        # Make sure there's at least `per_node_beam_size` options to be selected.\n",
        "        if not self.with_replacement:\n",
        "            exclusion_mask[..., :per_node_beam_size] = False\n",
        "\n",
        "        log_probs_descending[exclusion_mask] = torch.finfo(log_probs.dtype).min\n",
        "\n",
        "        # Now re-normalized the included log probs.\n",
        "        # shape: (batch_size, num_classes)\n",
        "        filtered_probabilities = torch.nn.functional.softmax(log_probs_descending, dim=-1)\n",
        "\n",
        "        # Sample from the re-normalized subset.\n",
        "        # NOTE: These indices are not indices into `log_probs`, they are indices into `log_probs_descending`.\n",
        "        # shape: (batch_size, per_node_beam_size)\n",
        "        sampled_indices = torch.multinomial(\n",
        "            filtered_probabilities, per_node_beam_size, replacement=self.with_replacement\n",
        "        )\n",
        "\n",
        "        # Convert `sampled_indices` back to indices in the original `log_probs` tensor.\n",
        "        # shape: (batch_size, per_node_beam_size)\n",
        "        selected_indices = sorting_indices.gather(-1, sampled_indices)\n",
        "\n",
        "        # Return (selected log probabilities, selected classes)\n",
        "        # shape: (len(log_probs),1) , (len(log_probs), 1)\n",
        "        return torch.gather(log_probs, 1, selected_indices), selected_indices, state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "z7h_nG8Nyhfx"
      },
      "outputs": [],
      "source": [
        "class FinalSequenceScorer:\n",
        "\n",
        "    @abstractmethod\n",
        "    def score(self, predictions: torch.Tensor, log_probabilities: torch.Tensor, end_index: int) -> torch.Tensor:\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lboXIC0JyojR"
      },
      "outputs": [],
      "source": [
        "class LengthNormalizedSequenceLogProbabilityScorer(FinalSequenceScorer):\n",
        "\n",
        "    def __init__(self, length_penalty: float = 1.0):\n",
        "        super().__init__()\n",
        "        self.length_penalty = length_penalty\n",
        "\n",
        "    def score(self, predictions: torch.Tensor, log_probabilities: torch.Tensor, end_index: int) -> torch.Tensor:\n",
        "        # shape: (batch_size, beam_size)\n",
        "        lengths = (predictions != end_index).long().sum(dim=2)\n",
        "\n",
        "        # If the sequence ended during beam search, the `log_probabilities` will include\n",
        "        # the transition to the end token. Therefore, in such situations, `lengths` is\n",
        "        # actually off by 1. This corrects for that.\n",
        "        # shape: (batch_size, beam_size)\n",
        "        is_end_token = predictions[:, :, -1] == end_index\n",
        "        lengths += is_end_token.long()\n",
        "\n",
        "        # shape: (batch_size, beam_size)\n",
        "        average_log_probs = log_probabilities / (lengths**self.length_penalty)\n",
        "        return average_log_probs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KbwPx5BByu9k"
      },
      "outputs": [],
      "source": [
        "class Constraint:\n",
        "\n",
        "\n",
        "    @abstractmethod\n",
        "    def init_state(\n",
        "        self,\n",
        "        batch_size: int,\n",
        "    ) -> ConstraintStateType:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def apply(\n",
        "        self,\n",
        "        state: ConstraintStateType,\n",
        "        class_log_probabilities: torch.Tensor,\n",
        "    ) -> torch.Tensor:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @staticmethod\n",
        "    def _copy_state(\n",
        "        state: ConstraintStateType,\n",
        "        batch_size: int,\n",
        "        beam_size: int,\n",
        "        last_backpointer: Optional[torch.Tensor] = None,\n",
        "    ) -> ConstraintStateType:\n",
        "\n",
        "        new_state = []\n",
        "        for i in range(batch_size):\n",
        "            batch_state = []\n",
        "            for j in range(beam_size):\n",
        "                if last_backpointer is None:\n",
        "                    # This is the first prediction, so the backpointer is 0\n",
        "                    backpointer = 0\n",
        "                else:\n",
        "                    backpointer = last_backpointer[i, j].item()\n",
        "                batch_state.append(copy.deepcopy(state[i][backpointer]))  # type: ignore\n",
        "            new_state.append(batch_state)\n",
        "        return new_state\n",
        "\n",
        "    def update_state(\n",
        "        self,\n",
        "        state: ConstraintStateType,\n",
        "        last_prediction: torch.Tensor,\n",
        "        last_backpointer: Optional[torch.Tensor] = None,\n",
        "    ) -> ConstraintStateType:\n",
        "        batch_size, beam_size = last_prediction.size()\n",
        "        new_state = self._copy_state(state, batch_size, beam_size, last_backpointer)\n",
        "        return self._update_state(new_state, last_prediction)\n",
        "\n",
        "    @abstractmethod\n",
        "    def _update_state(\n",
        "        self,\n",
        "        state: ConstraintStateType,\n",
        "        last_prediction: torch.Tensor,\n",
        "    ) -> ConstraintStateType:\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FFOi-ig9zAi5"
      },
      "outputs": [],
      "source": [
        "class RepeatedNGramBlockingConstraint(Constraint):\n",
        "    def __init__(self, ngram_size: int, **kwargs) -> None:\n",
        "        super().__init__(**kwargs)\n",
        "        self.ngram_size = ngram_size\n",
        "\n",
        "    def init_state(\n",
        "        self,\n",
        "        batch_size: int,\n",
        "    ) -> ConstraintStateType:\n",
        "        return [[{\"seen_ngrams\": {}, \"current_prefix\": []}] for _ in range(batch_size)]\n",
        "\n",
        "    def apply(\n",
        "        self,\n",
        "        state: ConstraintStateType,\n",
        "        class_log_probabilities: torch.Tensor,\n",
        "    ) -> torch.Tensor:\n",
        "        for i, batch in enumerate(state):\n",
        "            for j, beam in enumerate(batch):\n",
        "                current_prefix = tuple(beam[\"current_prefix\"])\n",
        "                seen_ngrams = beam[\"seen_ngrams\"]\n",
        "                try:\n",
        "                    disallowed_indices = seen_ngrams[current_prefix]\n",
        "                    class_log_probabilities[i, j, disallowed_indices] = torch.finfo(\n",
        "                        class_log_probabilities.dtype\n",
        "                    ).min\n",
        "                except KeyError:\n",
        "                    # We have not seen this prefix before, so there is no index\n",
        "                    # that needs to be blocked\n",
        "                    pass\n",
        "        return class_log_probabilities\n",
        "\n",
        "    def _update_state(\n",
        "        self,\n",
        "        state: ConstraintStateType,\n",
        "        last_prediction: torch.Tensor,\n",
        "    ) -> ConstraintStateType:\n",
        "        for i, batch in enumerate(state):\n",
        "            for j, beam in enumerate(batch):\n",
        "                prediction = last_prediction[i, j].item()\n",
        "                prefix = beam[\"current_prefix\"]\n",
        "                seen_ngrams = beam[\"seen_ngrams\"]\n",
        "\n",
        "                if len(prefix) == self.ngram_size - 1:\n",
        "                    # This is a new ngram that we have to remember\n",
        "                    if tuple(prefix) not in seen_ngrams:\n",
        "                        seen_ngrams[tuple(prefix)] = []\n",
        "                    seen_ngrams[tuple(prefix)].append(prediction)\n",
        "\n",
        "                # Create the new prefix, removing the oldest index if the prefix\n",
        "                # is too long\n",
        "                prefix.append(prediction)\n",
        "                if len(prefix) == self.ngram_size:\n",
        "                    prefix.pop(0)\n",
        "        return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rmLgJMgBzFqM"
      },
      "outputs": [],
      "source": [
        "class BeamSearch:\n",
        "\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        end_index: int,\n",
        "        *,\n",
        "        max_steps: int = 50,\n",
        "        beam_size: int = 10,\n",
        "        per_node_beam_size: Optional[int] = None,\n",
        "        sampler: Optional[Sampler] = None,\n",
        "        min_steps: Optional[int] = None,\n",
        "        final_sequence_scorer: Optional[FinalSequenceScorer] = None,\n",
        "        constraints: Optional[List[Constraint]] = None,\n",
        "    ) -> None:\n",
        "        if not max_steps > 0:\n",
        "            raise ValueError(\"max_steps must be positive\")\n",
        "        if not beam_size > 0:\n",
        "            raise ValueError(\"beam_size must be positive\")\n",
        "        if per_node_beam_size is not None and not per_node_beam_size > 0:\n",
        "            raise ValueError(\"per_node_beam_size must be positive\")\n",
        "        if min_steps is not None:\n",
        "            if not min_steps >= 0:\n",
        "                raise ValueError(\"min_steps must be non-negative\")\n",
        "            if not min_steps <= max_steps:\n",
        "                raise ValueError(\"min_steps must be less than or equal to max_steps\")\n",
        "\n",
        "        self._end_index = end_index\n",
        "        self.max_steps = max_steps\n",
        "        self.beam_size = beam_size\n",
        "        self.per_node_beam_size = per_node_beam_size or beam_size\n",
        "        self.sampler = sampler\n",
        "        self.min_steps = min_steps or 0\n",
        "        self.final_sequence_scorer = final_sequence_scorer\n",
        "        self.constraints = [constraints] or []\n",
        "\n",
        "    @staticmethod\n",
        "    def _reconstruct_sequences(predictions, backpointers):\n",
        "        # Reconstruct the sequences.\n",
        "        # shape: [(batch_size, beam_size, 1)]\n",
        "        reconstructed_predictions = [predictions[-1].unsqueeze(2)]\n",
        "\n",
        "        if not backpointers:\n",
        "            return reconstructed_predictions\n",
        "\n",
        "        # shape: (batch_size, beam_size)\n",
        "        cur_backpointers = backpointers[-1]\n",
        "\n",
        "        for timestep in range(len(predictions) - 2, 0, -1):\n",
        "            # shape: (batch_size, beam_size, 1)\n",
        "            cur_preds = predictions[timestep].gather(1, cur_backpointers).unsqueeze(2)\n",
        "\n",
        "            reconstructed_predictions.append(cur_preds)\n",
        "\n",
        "            # shape: (batch_size, beam_size)\n",
        "            cur_backpointers = backpointers[timestep - 1].gather(1, cur_backpointers)\n",
        "\n",
        "        # shape: (batch_size, beam_size, 1)\n",
        "        final_preds = predictions[0].gather(1, cur_backpointers).unsqueeze(2)\n",
        "\n",
        "        reconstructed_predictions.append(final_preds)\n",
        "\n",
        "        return reconstructed_predictions\n",
        "\n",
        "    def search(\n",
        "        self,\n",
        "        start_predictions: torch.Tensor,\n",
        "        start_state: StateType,\n",
        "        step: StepFunctionType,\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "\n",
        "        step_signature = signature(step)\n",
        "        if len(step_signature.parameters) < 3:\n",
        "            # If the step function we're given does not take the time step argument, wrap it\n",
        "            # in one that does.\n",
        "            old_step = cast(StepFunctionTypeNoTimestep, step)\n",
        "\n",
        "            def new_step(last_predictions: torch.Tensor, state: Dict[str, torch.Tensor], time_step: int):\n",
        "                del time_step\n",
        "                return old_step(last_predictions, state)\n",
        "\n",
        "            return self._search(start_predictions, start_state, new_step)\n",
        "        else:\n",
        "            return self._search(start_predictions, start_state, cast(StepFunctionTypeWithTimestep, step))\n",
        "\n",
        "    def _search(\n",
        "        self,\n",
        "        start_predictions: torch.Tensor,\n",
        "        start_state: StateType,\n",
        "        step: StepFunctionTypeWithTimestep,\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        batch_size = start_predictions.size()[0]\n",
        "\n",
        "        # List of (batch_size, beam_size) tensors. One for each time step. Does not\n",
        "        # include the start symbols, which are implicit.\n",
        "        predictions: List[torch.Tensor] = []\n",
        "\n",
        "        # List of (batch_size, beam_size) tensors. One for each time step. None for\n",
        "        # the first.  Stores the index n for the parent prediction, i.e.\n",
        "        # predictions[t-1][i][n], that it came from.\n",
        "        backpointers: List[torch.Tensor] = []\n",
        "\n",
        "        constraint_states = [constraint.init_state(batch_size) for constraint in self.constraints]\n",
        "\n",
        "        # Calculate the first timestep. This is done outside the main loop\n",
        "        # because we are going from a single decoder input (the output from the\n",
        "        # encoder) to the top `beam_size` decoder outputs. On the other hand,\n",
        "        # within the main loop we are going from the `beam_size` elements of the\n",
        "        # beam to `beam_size`^2 candidates from which we will select the top\n",
        "        # `beam_size` elements for the next iteration.\n",
        "        # shape: (batch_size, num_classes)\n",
        "        start_class_log_probabilities, state = step(start_predictions, start_state, 0)\n",
        "\n",
        "        num_classes = start_class_log_probabilities.size()[1]\n",
        "\n",
        "        # Make sure `per_node_beam_size` is not larger than `num_classes`.\n",
        "        if self.per_node_beam_size > num_classes:\n",
        "            raise ValueError(\n",
        "                f\"Vocab size ({num_classes:d}) too small \"\n",
        "                f\"relative to per_node_beam_size ({self.per_node_beam_size:d}).\\n\"\n",
        "                f\"Please decrease beam_size or per_node_beam_size.\"\n",
        "            )\n",
        "\n",
        "        sampler_state = self.sampler.init_state(start_class_log_probabilities, batch_size, num_classes)\n",
        "\n",
        "        # Apply all constraints.\n",
        "        if self.constraints:\n",
        "            # shape: (batch_size, 1, num_classes)\n",
        "            expanded_start_class_log_probabilities = start_class_log_probabilities.unsqueeze(1)\n",
        "            for constraint, constraint_state in zip(self.constraints, constraint_states):\n",
        "                expanded_start_class_log_probabilities = constraint.apply(\n",
        "                    constraint_state, expanded_start_class_log_probabilities\n",
        "                )\n",
        "            start_class_log_probabilities = expanded_start_class_log_probabilities.squeeze(1)\n",
        "\n",
        "        # Prevent selecting the end symbol if there is any min_steps constraint\n",
        "        if self.min_steps >= 1:\n",
        "            start_class_log_probabilities[:, self._end_index] = torch.finfo(\n",
        "                start_class_log_probabilities.dtype\n",
        "            ).min\n",
        "\n",
        "        # Get the initial predicted classed and their log probabilities.\n",
        "        # shape: (batch_size, beam_size), (batch_size, beam_size)\n",
        "        (\n",
        "            start_top_log_probabilities,\n",
        "            start_predicted_classes,\n",
        "            sampler_state,\n",
        "        ) = self.sampler.sample_beams(start_class_log_probabilities, self.beam_size, sampler_state)\n",
        "\n",
        "        if self.beam_size == 1 and (start_predicted_classes == self._end_index).all():\n",
        "            warnings.warn(\n",
        "                \"Empty sequences predicted. You may want to increase the beam size or ensure \"\n",
        "                \"your step function is working properly.\",\n",
        "                RuntimeWarning,\n",
        "            )\n",
        "            return start_predicted_classes.unsqueeze(-1), start_top_log_probabilities\n",
        "\n",
        "        # The log probabilities for the last time step.\n",
        "        # shape: (batch_size, beam_size)\n",
        "        last_log_probabilities = start_top_log_probabilities\n",
        "\n",
        "        # shape: [(batch_size, beam_size)]\n",
        "        predictions.append(start_predicted_classes)\n",
        "\n",
        "        # Log probability tensor that mandates that the end token is selected.\n",
        "        # shape: (batch_size * beam_size, num_classes)\n",
        "        log_probs_after_end = start_class_log_probabilities.new_full(\n",
        "            (batch_size * self.beam_size, num_classes),\n",
        "            torch.finfo(start_class_log_probabilities.dtype).min,\n",
        "        )\n",
        "        log_probs_after_end[:, self._end_index] = 0.0\n",
        "\n",
        "        # Set the same state for each element in the beam.\n",
        "        self._update_initial_state(state, batch_size)\n",
        "\n",
        "        for i, constraint in enumerate(self.constraints):\n",
        "            constraint_states[i] = constraint.update_state(constraint_states[i], start_predicted_classes)\n",
        "\n",
        "        for timestep in range(self.max_steps - 1):\n",
        "            # shape: (batch_size * beam_size,)\n",
        "            last_predictions = predictions[-1].reshape(batch_size * self.beam_size)\n",
        "\n",
        "            # If every predicted token from the last step is `self._end_index`,\n",
        "            # then we can stop early.\n",
        "            if (last_predictions == self._end_index).all():\n",
        "                break\n",
        "            # Take a step. This get the predicted log probs of the next classes\n",
        "            # and updates the state.\n",
        "            # shape: (batch_size * beam_size, num_classes)\n",
        "            class_log_probabilities, state = step(last_predictions, state, timestep + 1)\n",
        "\n",
        "            # Apply all constraints.\n",
        "            if self.constraints:\n",
        "                # shape: (batch_size, beam_size, num_classes)\n",
        "                reshaped_class_log_probabilities = class_log_probabilities.view(batch_size, self.beam_size, -1)\n",
        "                for constraint, constraint_state in zip(self.constraints, constraint_states):\n",
        "                    reshaped_class_log_probabilities = constraint.apply(\n",
        "                        constraint_state, reshaped_class_log_probabilities\n",
        "                    )\n",
        "                # shape: (batch_size * beam_size, num_classes)\n",
        "                class_log_probabilities = reshaped_class_log_probabilities.view(batch_size * self.beam_size, -1)\n",
        "\n",
        "            # The `timestep`-th iteration of the for loop is generating the `timestep + 2`-th token\n",
        "            # of the sequence (because `timestep` is 0-indexed and we generated the first token\n",
        "            # before the for loop). Here we block the end index if the search is not allowed to\n",
        "            # terminate on this iteration.\n",
        "            if timestep + 2 <= self.min_steps:\n",
        "                class_log_probabilities[:, self._end_index] = torch.finfo(class_log_probabilities.dtype).min\n",
        "\n",
        "            # shape: (batch_size * beam_size, num_classes)\n",
        "            last_predictions_expanded = last_predictions.unsqueeze(-1).expand(\n",
        "                batch_size * self.beam_size, num_classes\n",
        "            )\n",
        "\n",
        "            # Here we are finding any beams where we predicted the end token in\n",
        "            # the previous timestep and replacing the distribution with a\n",
        "            # one-hot distribution, forcing the beam to predict the end token\n",
        "            # this timestep as well.if padding_len > 0:\n",
        "            # shape: (batch_size * beam_size, num_classes)\n",
        "            cleaned_log_probabilities = torch.where(\n",
        "                last_predictions_expanded == self._end_index,\n",
        "                log_probs_after_end,\n",
        "                class_log_probabilities,\n",
        "            )\n",
        "\n",
        "            # shape (both): (batch_size * beam_size, per_node_beam_size)\n",
        "            top_log_probabilities, predicted_classes, sampler_state = self.sampler.sample_nodes(\n",
        "                cleaned_log_probabilities, self.per_node_beam_size, sampler_state\n",
        "            )\n",
        "\n",
        "            # Here we expand the last log probabilities to (batch_size * beam_size, per_node_beam_size)\n",
        "            # so that we can add them to the current log probs for this timestep.\n",
        "            # This lets us maintain the log probability of each element on the beam.\n",
        "            # shape: (batch_size * beam_size, per_node_beam_size)\n",
        "            expanded_last_log_probabilities = (\n",
        "                last_log_probabilities.unsqueeze(2)\n",
        "                .expand(batch_size, self.beam_size, self.per_node_beam_size)\n",
        "                .reshape(batch_size * self.beam_size, self.per_node_beam_size)\n",
        "            )\n",
        "\n",
        "            # shape: (batch_size * beam_size, per_node_beam_size)\n",
        "            summed_top_log_probabilities = top_log_probabilities + expanded_last_log_probabilities\n",
        "\n",
        "            # shape: (batch_size, beam_size * per_node_beam_size)\n",
        "            reshaped_summed = summed_top_log_probabilities.reshape(\n",
        "                batch_size, self.beam_size * self.per_node_beam_size\n",
        "            )\n",
        "\n",
        "            # shape: (batch_size, beam_size * per_node_beam_size)\n",
        "            reshaped_predicted_classes = predicted_classes.reshape(\n",
        "                batch_size, self.beam_size * self.per_node_beam_size\n",
        "            )\n",
        "\n",
        "            # Keep only the top `beam_size` beam indices.\n",
        "            # shape (both): (batch_size, beam_size)\n",
        "            (\n",
        "                restricted_beam_log_probs,\n",
        "                restricted_beam_indices,\n",
        "                sampler_state,\n",
        "            ) = self.sampler.sample_beams(reshaped_summed, self.beam_size, sampler_state)\n",
        "\n",
        "            # Use the beam indices to extract the corresponding classes.\n",
        "            # shape: (batch_size, beam_size)\n",
        "            restricted_predicted_classes = reshaped_predicted_classes.gather(1, restricted_beam_indices)\n",
        "\n",
        "            predictions.append(restricted_predicted_classes)\n",
        "\n",
        "            # shape: (batch_size, beam_size)\n",
        "            last_log_probabilities = restricted_beam_log_probs\n",
        "\n",
        "            # The beam indices come from a `beam_size * per_node_beam_size` dimension where the\n",
        "            # indices with a common ancestor are grouped together. Hence\n",
        "            # dividing by per_node_beam_size gives the ancestor. (Note that this is integer\n",
        "            # division as the tensor is a LongTensor.)\n",
        "            # shape: (batch_size, beam_size)\n",
        "            backpointer = torch.divide(restricted_beam_indices, self.per_node_beam_size, rounding_mode=\"trunc\")\n",
        "            backpointers.append(backpointer)\n",
        "\n",
        "            # Keep only the pieces of the state tensors corresponding to the\n",
        "            # ancestors created this iteration.\n",
        "            self._update_state(state, backpointer)\n",
        "\n",
        "            for i, constraint in enumerate(self.constraints):\n",
        "                constraint_states[i] = constraint.update_state(\n",
        "                    constraint_states[i], restricted_predicted_classes, last_backpointer=backpointer\n",
        "                )\n",
        "\n",
        "        # Warn about \"-inf\" log probabilities if not using any constraints (negligible\n",
        "        # log probabilities are expected when using constraints).\n",
        "        if not self.constraints and (\n",
        "            not torch.isfinite(last_log_probabilities).all()\n",
        "            or (last_log_probabilities == torch.finfo(last_log_probabilities.dtype).min).any()\n",
        "        ):\n",
        "            warnings.warn(\n",
        "                \"Negligible log probabilities encountered ('-inf' or equivalent). \"\n",
        "                \"Some final sequences may not make sense. \"\n",
        "                \"This can happen when the beam size is larger than the number of valid (non-zero \"\n",
        "                \"probability) transitions that the step function produces.\",\n",
        "                RuntimeWarning,\n",
        "            )\n",
        "\n",
        "        reconstructed_predictions = self._reconstruct_sequences(predictions, backpointers)\n",
        "\n",
        "        # shape: (batch_size, beam_size, max_steps)\n",
        "        all_predictions = torch.cat(list(reversed(reconstructed_predictions)), 2)\n",
        "\n",
        "        # Calculate the final sequence scores\n",
        "        # shape: (batch_size, beam_size)\n",
        "        final_scores = self.final_sequence_scorer.score(all_predictions, last_log_probabilities, self._end_index)\n",
        "\n",
        "        # Sort the sequences based on the final scores so the best scoring\n",
        "        # sequence is at index 0\n",
        "        sorted_final_scores, sorted_indices = torch.sort(final_scores, dim=1, descending=True)\n",
        "        sorted_all_predictions = torch.gather(\n",
        "            all_predictions, 1, sorted_indices.unsqueeze(-1).expand_as(all_predictions)\n",
        "        )\n",
        "\n",
        "        return sorted_all_predictions, sorted_final_scores\n",
        "\n",
        "    def _update_initial_state(self, state: StateType, batch_size: int):\n",
        "        \"\"\"\n",
        "        Expand tensors in a state dictionary from `(batch_size, *)` to `(batch_size * beam_size, *)`.\n",
        "        \"\"\"\n",
        "        for key, state_tensor in state.items():\n",
        "            if state_tensor is None:\n",
        "                continue\n",
        "            # shape: (batch_size * beam_size, *)\n",
        "            _, *last_dims = state_tensor.size()\n",
        "            state[key] = (\n",
        "                state_tensor.unsqueeze(1)\n",
        "                .expand(batch_size, self.beam_size, *last_dims)\n",
        "                .reshape(batch_size * self.beam_size, *last_dims)\n",
        "            )\n",
        "\n",
        "    def _update_state(self, state: StateType, backpointer: torch.Tensor):\n",
        "        batch_size = backpointer.size()[0]\n",
        "\n",
        "        for key, state_tensor in state.items():\n",
        "            if state_tensor is None:\n",
        "                continue\n",
        "            _, *last_dims = state_tensor.size()\n",
        "            # shape: (batch_size, beam_size, *)\n",
        "            expanded_backpointer = backpointer.view(batch_size, self.beam_size, *([1] * len(last_dims))).expand(\n",
        "                batch_size, self.beam_size, *last_dims\n",
        "            )\n",
        "            # shape: (batch_size * beam_size, *)\n",
        "            state[key] = (\n",
        "                state_tensor.reshape(batch_size, self.beam_size, *last_dims)\n",
        "                .gather(1, expanded_backpointer)\n",
        "                .reshape(batch_size * self.beam_size, *last_dims)\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SzvrgAk3JAYy"
      },
      "outputs": [],
      "source": [
        "# def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
        "#     sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
        "#     eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
        "\n",
        "#     # Precompute the encoder output and reuse it for every step\n",
        "#     encoder_output = model.encode(source, source_mask)\n",
        "#     # Initialize the decoder input with the sos token\n",
        "#     decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
        "#     while True:\n",
        "#         if decoder_input.size(1) == max_len:\n",
        "#             break\n",
        "\n",
        "#         # build mask for target\n",
        "#         decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
        "\n",
        "#         # calculate output\n",
        "#         out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
        "\n",
        "#         # get next token\n",
        "#         prob = model.project(out[:, -1])\n",
        "#         _, next_word = torch.max(prob, dim=1)\n",
        "#         decoder_input = torch.cat(\n",
        "#             [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
        "#         )\n",
        "\n",
        "#         if next_word == eos_idx:\n",
        "#             break\n",
        "\n",
        "#     return decoder_input.squeeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RRbL1TK04ccM"
      },
      "outputs": [],
      "source": [
        "torch.set_float32_matmul_precision('high')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gReSKEFnJExB"
      },
      "outputs": [],
      "source": [
        "\n",
        "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_step, num_examples=2):\n",
        "    model.eval()\n",
        "    count = 0\n",
        "    state = {}\n",
        "\n",
        "    source_texts = []\n",
        "    expected = []\n",
        "    predicted = []\n",
        "\n",
        "    sos_idx = tokenizer_tgt.token_to_id('<sos>')\n",
        "    eos_idx = tokenizer_tgt.token_to_id('<eos>')\n",
        "\n",
        "    try:\n",
        "        # get the console window width\n",
        "        with os.popen('stty size', 'r') as console:\n",
        "            _, console_width = console.read().split()\n",
        "            console_width = int(console_width)\n",
        "    except:\n",
        "        # If we can't get the console width, use 80 as default\n",
        "        console_width = 80\n",
        "\n",
        "    beam_search = BeamSearch(\n",
        "            eos_idx,\n",
        "            max_steps=610,\n",
        "            beam_size=3,\n",
        "            per_node_beam_size=3,\n",
        "            sampler=TopPSampler(),\n",
        "            min_steps=5,\n",
        "            final_sequence_scorer=LengthNormalizedSequenceLogProbabilityScorer(),\n",
        "            constraints=RepeatedNGramBlockingConstraint(3),\n",
        "        )\n",
        "\n",
        "\n",
        "    def step(\n",
        "            last_predictions: torch.Tensor, state: dict[str, torch.Tensor]\n",
        "        ) -> tuple[torch.Tensor, dict[str, torch.Tensor]]:\n",
        "            nonlocal tokens_generated\n",
        "\n",
        "            attention_mask = state.get(\"attention_mask\")\n",
        "            if tokens_generated > 0:\n",
        "                input_ids = last_predictions.unsqueeze(1)\n",
        "                if attention_mask is not None:\n",
        "                    group_size = input_ids.shape[0]\n",
        "                    attention_mask = torch.cat((attention_mask, attention_mask.new_ones((group_size, 1))), dim=-1)\n",
        "            else:\n",
        "                past_key_values = None\n",
        "                input_ids = last_predictions\n",
        "\n",
        "            tokens_generated += 1\n",
        "\n",
        "            # Run forward pass of model to get logits, then normalize to get log probs.\n",
        "            output = model.decode(encoder_output, encoder_mask, input_ids, attention_mask)\n",
        "            output = model.project(output)\n",
        "            log_probs = F.log_softmax(output[:, -1, :], dim=-1)\n",
        "\n",
        "            # Create new state.\n",
        "            if attention_mask is not None:\n",
        "                state[\"attention_mask\"] = attention_mask\n",
        "\n",
        "            return log_probs, state\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in validation_ds:\n",
        "            tokens_generated = 0\n",
        "            count += 1\n",
        "            encoder_input = batch[\"encoder_input\"].to(device) # (b, seq_len)\n",
        "            encoder_mask = batch[\"encoder_mask\"].to(device) # (b, 1, 1, seq_len)\n",
        "\n",
        "            # check that the batch size is 1\n",
        "            assert encoder_input.size(\n",
        "                0) == 1, \"Batch size must be 1 for validation\"\n",
        "\n",
        "            # Precompute the encoder output and reuse it for every step\n",
        "            encoder_output = model.encode(encoder_input,encoder_mask)\n",
        "            # Initialize the decoder input with the sos token\n",
        "\n",
        "            initial_preds = torch.empty(1,1).fill_(sos_idx).type_as(encoder_input).to(device)\n",
        "            decoder_mask = causal_mask(initial_preds.size(1)).type_as(encoder_mask ).to(device)\n",
        "            state[\"attention_mask\"] = decoder_mask\n",
        "\n",
        "            state: dict[str, torch.Tensor] = {\"input_ids\": initial_preds}\n",
        "\n",
        "\n",
        "            model_out, scores = beam_search.search(initial_preds, state, step)\n",
        "\n",
        "            source_text = batch[\"src_text\"][0]\n",
        "            target_text = batch[\"tgt_text\"][0]\n",
        "\n",
        "\n",
        "            preds = [tokenizer_tgt.decode(preds[0]) for preds in model_out.detach().cpu().numpy()]\n",
        "            probs = [round(float(p[0]), 4) for p in scores.exp()]\n",
        "            #model_out_text = tokenizer_tgt.decode_batch(model_out.detach().cpu().numpy())\n",
        "\n",
        "            source_texts.append(source_text)\n",
        "            expected.append(target_text)\n",
        "            #predicted.append(model_out_text)\n",
        "\n",
        "            # Print the source, target and model output\n",
        "            print_msg('-'*console_width)\n",
        "            print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n",
        "            print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n",
        "            print_msg(f\"{f'PREDICTED: ':>12}{preds}\")\n",
        "            print_msg(f\"{f'SCORE: ':>12}{probs}\")\n",
        "\n",
        "            if count == num_examples:\n",
        "                print_msg('-'*console_width)\n",
        "                break\n",
        "\n",
        "    # Evaluate the character error rate\n",
        "    # Compute the char error rate\n",
        "    metric = torchmetrics.CharErrorRate()\n",
        "    cer = metric(predicted, expected)\n",
        "    wandb.log({'validation/cer': cer, 'global_step': global_step})\n",
        "\n",
        "    # Compute the word error rate\n",
        "    metric = torchmetrics.WordErrorRate()\n",
        "    wer = metric(predicted, expected)\n",
        "    wandb.log({'validation/wer': wer, 'global_step': global_step})\n",
        "\n",
        "    # Compute the BLEU metric\n",
        "    metric = torchmetrics.BLEUScore()\n",
        "    bleu = metric(predicted, expected)\n",
        "    wandb.log({'validation/BLEU': bleu, 'global_step': global_step})\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KE2pC-Be-57n"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "class CustomLRAdamOptimizer:\n",
        "    def __init__(self, optimizer, model_dimension, num_of_warmup_steps):\n",
        "        self.optimizer = optimizer\n",
        "        self.model_size = model_dimension\n",
        "        self.num_of_warmup_steps = num_of_warmup_steps\n",
        "        self.current_step_number = 0\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        self.current_step_number += 1\n",
        "        current_learning_rate = self.get_current_learning_rate()\n",
        "\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr'] = current_learning_rate\n",
        "\n",
        "        return self.optimizer.step(closure)\n",
        "\n",
        "    def state_dict(self):\n",
        "        return {\n",
        "            'custom_state': {key: value for key, value in self.__dict__.items() if key != 'optimizer'},\n",
        "            'optimizer_state_dict': self.optimizer.state_dict()\n",
        "        }\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        self.__dict__.update(state_dict['custom_state'])\n",
        "        self.optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
        "\n",
        "    def get_current_learning_rate(self):\n",
        "        step = self.current_step_number\n",
        "        warmup = self.num_of_warmup_steps\n",
        "\n",
        "        # Warmup phase\n",
        "        if step <= warmup:\n",
        "            return (self.model_size ** (-0.5)) * (step * (warmup ** (-1.5)))\n",
        "\n",
        "        # Decay phase (inverse square root)\n",
        "        return (self.model_size ** (-0.5)) * (warmup ** 0.5) * (step ** (-0.5))\n",
        "\n",
        "    @property\n",
        "    def param_groups(self):\n",
        "        return self.optimizer.param_groups\n",
        "\n",
        "    def zero_grad(self, set_to_none=False):\n",
        "        self.optimizer.zero_grad(set_to_none=set_to_none)\n",
        "\n",
        "    # Pass through methods to the underlying optimizer\n",
        "    def __getattr__(self, name):\n",
        "        return getattr(self.optimizer, name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JifRofbQoEs4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "import math\n",
        "\n",
        "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
        "    model = build_transformer(vocab_src_len, vocab_tgt_len, d_model=config['d_model'])\n",
        "    return model\n",
        "\n",
        "def train_model(config):\n",
        "    # Define the device\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n",
        "    print(\"Using device:\", device)\n",
        "    if (device == 'cuda'):\n",
        "        print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
        "        print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
        "    elif (device == 'mps'):\n",
        "        print(f\"Device name: <mps>\")\n",
        "    else:\n",
        "        print(\"NOTE: If you have a GPU, consider using it for training.\")\n",
        "    device = torch.device(device)\n",
        "\n",
        "    # Make sure the weights folder exists\n",
        "    Path(f\"{config['datasource']}_{config['model_folder']}\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt, dict_plot = get_ds(config)\n",
        "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
        "    model = torch.compile(model)  # Using torch.compile for potential speedup\n",
        "\n",
        "    MODEL_DIMENSION = 768\n",
        "    num_warmup_steps = 4000\n",
        "    trg_vocab_size = tokenizer_tgt.get_vocab_size()\n",
        "    pad_token_id = torch.tensor([tokenizer_tgt.token_to_id(\"<pad>\")], dtype=torch.int64)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=0.0, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.01)\n",
        "    custom_lr_optimizer = CustomLRAdamOptimizer(optimizer, MODEL_DIMENSION, num_warmup_steps)\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id(\"<pad>\"), label_smoothing=0.1).to(device)\n",
        "\n",
        "    initial_epoch = 0\n",
        "    global_step = 0\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    # Preload model if specified\n",
        "    preload = config['preload']\n",
        "    model_filename = latest_weights_file_path(config) if preload == 'latest' else get_weights_file_path(config, preload) if preload else None\n",
        "    if model_filename:\n",
        "        print(f'Preloading model {model_filename}')\n",
        "        state = torch.load(model_filename)\n",
        "        model.load_state_dict(state['model_state_dict'])\n",
        "        initial_epoch = state['epoch'] + 1\n",
        "        custom_lr_optimizer.load_state_dict(state['optimizer_state_dict'])\n",
        "        global_step = state['global_step']\n",
        "    else:\n",
        "        print('No model to preload, starting from scratch')\n",
        "\n",
        "    # define our custom x axis metric\n",
        "    wandb.define_metric(\"global_step\")\n",
        "    # define which metrics will be plotted against it\n",
        "    wandb.define_metric(\"validation/*\", step_metric=\"global_step\")\n",
        "    wandb.define_metric(\"train/*\", step_metric=\"global_step\")\n",
        "\n",
        "    # Add this before the training loop\n",
        "    balance_loss_weight = 0.005  # Adjust this value as needed\n",
        "\n",
        "    for epoch in range(initial_epoch, config['num_epochs']):\n",
        "        model.train()\n",
        "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
        "\n",
        "        for batch in batch_iterator:\n",
        "            encoder_input = batch['encoder_input'].to(device)\n",
        "            decoder_input = batch['decoder_input'].to(device)\n",
        "            encoder_mask = batch['encoder_mask'].to(device)\n",
        "            decoder_mask = batch['decoder_mask'].to(device)\n",
        "            label = batch['label'].to(device)\n",
        "\n",
        "            with autocast():\n",
        "                encoder_output = model.encode(encoder_input, encoder_mask)\n",
        "                decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
        "                proj_output = model.project(decoder_output)\n",
        "\n",
        "                # Compute the main loss\n",
        "                main_loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
        "\n",
        "                # Collect balance losses from all MoE layers\n",
        "                balance_losses = []\n",
        "                for module in model.modules():\n",
        "                    if hasattr(module, 'get_balance_loss'):\n",
        "                        balance_losses.append(module.get_balance_loss())\n",
        "\n",
        "                # Compute total balance loss\n",
        "                total_balance_loss = sum(balance_losses) if balance_losses else torch.tensor(0.0).to(device)\n",
        "\n",
        "                # Combine main loss and balance loss\n",
        "                loss = main_loss + balance_loss_weight * total_balance_loss\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(custom_lr_optimizer.optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            scaler.step(custom_lr_optimizer)\n",
        "            scaler.update()\n",
        "            custom_lr_optimizer.zero_grad()\n",
        "\n",
        "            batch_iterator.set_postfix({\n",
        "                \"loss\": f\"{loss.item():6.3f}\",\n",
        "                \"main_loss\": f\"{main_loss.item():6.3f}\",\n",
        "                \"balance_loss\": f\"{total_balance_loss.item():6.3f}\"\n",
        "            })\n",
        "            wandb.log({\n",
        "                'train/loss': loss.item(),\n",
        "                'train/main_loss': main_loss.item(),\n",
        "                'train/balance_loss': total_balance_loss.item(),\n",
        "                'train/learning_rate':  custom_lr_optimizer.get_current_learning_rate(),\n",
        "                'global_step': global_step\n",
        "            })\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step)\n",
        "\n",
        "        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': custom_lr_optimizer.state_dict(),\n",
        "            'global_step': global_step\n",
        "        }, model_filename)\n",
        "\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAhLouXqtpR-"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings(\"ignore\")\n",
        "config = get_config()\n",
        "wandb.init(\n",
        "        # set the wandb project where this run will be logged\n",
        "        project=\"pytorch-transformer-smoe\",\n",
        "\n",
        "        # track hyperparameters and run metadata\n",
        "        config=config\n",
        "    )\n",
        "dict_plot = train_model(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6y_Q49Ct_So3"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2t5AmRrcjCW"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UICVqmN9dFvm"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2,\n",
        "                           figsize=(14, 7))\n",
        "\n",
        "ax1.plot(dict_plot[\"src_len\"])\n",
        "ax1.axhline(y = 300, color = 'r', linestyle = '--')\n",
        "ax1.set_title('Scatter: $x$ versus $y$')\n",
        "ax1.set_xlabel('$x$')\n",
        "ax1.set_ylabel('$y$')\n",
        "\n",
        "ax2.plot(dict_plot[\"tgt_len\"])\n",
        "ax2.axhline(y = 300, color = 'r', linestyle = '--')\n",
        "ax2.legend(loc=(0.65, 0.8))\n",
        "ax2.set_title('Frequencies of $x$ and $y$')\n",
        "ax2.yaxis.tick_right()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7n3HaRAjnK3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nScBYSW6jfg-"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame.from_dict(dict_plot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BISpLDzBjvrn"
      },
      "outputs": [],
      "source": [
        "df.sample(8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_xYI9i7kAqH"
      },
      "outputs": [],
      "source": [
        "def mapper(example):\n",
        "  if (example[\"src_len\"] < 300 and example[\"tgt_len\"]<300):\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HdemTt-ksm-"
      },
      "outputs": [],
      "source": [
        "df[\"filtered\"] = df.apply(lambda x: mapper(x), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uirI6F_uljqY"
      },
      "outputs": [],
      "source": [
        "filtered_rows = df[df['filtered'] == 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJkr9FJWlv2p"
      },
      "outputs": [],
      "source": [
        "filtered_rows.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7cKPWJdlm2g"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2,\n",
        "                           figsize=(14, 7))\n",
        "\n",
        "ax1.plot(filtered_rows[\"src_len\"])\n",
        "ax1.axhline(y = 300, color = 'r', linestyle = '--')\n",
        "ax1.set_title('Scatter: $x$ versus $y$')\n",
        "ax1.set_xlabel('$x$')\n",
        "ax1.set_ylabel('$y$')\n",
        "\n",
        "ax2.plot(filtered_rows[\"tgt_len\"])\n",
        "ax2.axhline(y = 300, color = 'r', linestyle = '--')\n",
        "ax2.legend(loc=(0.65, 0.8))\n",
        "ax2.set_title('Frequencies of $x$ and $y$')\n",
        "ax2.yaxis.tick_right()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUnm1n5VmCM3"
      },
      "outputs": [],
      "source": [
        "len(filtered_rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yA3WmzHYvgaK"
      },
      "outputs": [],
      "source": [
        "filtered_rows[\"filtered\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDc__K6nv4NL"
      },
      "outputs": [],
      "source": [
        "filtered_rows.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUUoSIqtvpu6"
      },
      "outputs": [],
      "source": [
        "filtered_rows = filtered_rows.drop(columns=[\"src_len\", \"tgt_len\", \"filtered\"], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cvkibk56wCSq"
      },
      "outputs": [],
      "source": [
        "filtered_rows.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1awnDOf1wGsq"
      },
      "outputs": [],
      "source": [
        "filtered_rows.to_csv('translation_dataset.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8Wdyw2Ay6FD"
      },
      "outputs": [],
      "source": [
        "print(filtered_rows.iloc[4,:][\"src\"])\n",
        "print(filtered_rows.iloc[4,:][\"tgt\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30665,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}